<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content=
  "HTML Tidy for Windows (vers 7 December 2008), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link rel="stylesheet" type="text/css" href="../images/dvd.css">
  <script type="text/javascript">
              function myFunction(n) {
                    var id = n
                    var x = document.getElementById(id);
                    if (x.style.display === 'block') {
                        x.style.display = 'none';
                    } else {
                        x.style.display = 'block';
                    }
                }
  </script>

  <title>Eurographics / VGTC Conference on Visualization (EuroVis) - Posters
  2018</title>
</head>

<body>
  <div class="table1">
    <h1>Eurographics / VGTC Conference on Visualization (EuroVis) - Posters
    2018</h1>

    <div class="bibtex">
      <a href="../index.html">&lt;&lt;&lt; HOME</a>
    </div>

    <div style="clear:both;"></div>

    <div class="bibtex">
      <a href="bibtex.bib" target="_blank">Download BiBTeX (whole event)</a>
    </div>

    <div style="clear:both;"></div>
  </div><br>

  <div class="table2">
    <div class="other">
      <a href="000_eurovis2018posters_frontmatter.pdf">Table of Contents</a>
    </div>

    <div class="sectionheader">
      Posters
    </div>

    <div class="title">
      CV3: Visual Exploration, Assessment, and Comparison of CVs
    </div>

    <div class="links">
      [<a href="pdf/001-003.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="001-003.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Velitchko Andreev Filipov, Paolo Federico, and Silvia Miksch
    </div>

    <div class="button">
      <button onclick="myFunction(1)">+/-</button>
    </div>

    <div id='1' class="abstract">
      Curriculum Vitae (CV) is an established representation of a person's
      academic and professional history. A typical CV is comprised of multiple
      sections associated with spatial, temporal, nominal, and ordinal data.
      Commonly, comparing and assessing CVs is done by viewing them in a
      side-by-side fashion. This becomes challenging when comparing more than
      two CVs, because the reader is required to switch attention back and
      forth, the overview becomes cluttered, and assessing the CVs becomes a
      nontrivial task. In order to address this challenge, we propose the
      design and implementation of an interactive exploration environment
      capable of comparing multiple CVs, visualizing their information in a
      clear manner, whilst maintaining a clean overview. Our approach offers
      users a new way to explore, assess, and compare multiple CVs.
    </div>

    <div class="title">
      Extending Document Exploration with Image Retrieval: Concept and First
      Results
    </div>

    <div class="links">
      [<a href="pdf/005-007.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="005-007.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Lin Shao, Mathias Glatz, Eric Gergely, Markus M&uuml;ller, Denis Munter,
      Stefan Papst, and Tobias Schreck
    </div>

    <div class="button">
      <button onclick="myFunction(2)">+/-</button>
    </div>

    <div id='2' class="abstract">
      Information retrieval provides to date effective methods to search for
      documents relevant to user queries, and to support exploration of
      clusters of similar documents. Typically, the retrieval relies on
      text-based queries and similarity functions. However, in many cases also
      visual content is important in documents, for example, in the
      visualization field. There, researchers may want to search for papers
      based on similar example visualizations, which is difficult by relying on
      keyword search alone. We present a concept to automatically label
      visualization types in research papers and search for similar images,
      relying on state of the art image descriptors. We created a prototype
      that allows to search for papers showing images similar to a query image.
      Preliminary results of applying it on a corpus of VAST papers indicate
      the chosen descriptors can retrieve papers with similar images. Our
      approach for image-based search can complement text-based search and in
      perspective, support document corpus exploration based on clustering
      contained images. In future work, we want to explore if image-based
      search can also support the formation of taxonomies of a corpus or
      research papers, based on image similarity.
    </div>

    <div class="title">
      Visually Exploring Data Provenance and Quality of Open Data
    </div>

    <div class="links">
      [<a href="pdf/009-011.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="009-011.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Christian Bors, Theresia Gschwandtner, and Silvia Miksch
    </div>

    <div class="button">
      <button onclick="myFunction(3)">+/-</button>
    </div>

    <div id='3' class="abstract">
      While open data platforms are increasingly popular among end-users as
      well as data providers, there is a growing problem with inconsistent
      update frequencies and lack of quality in datasets. Efforts to monitor
      data quality are currently limited to checking meta-information and
      creating revisions to allow manual inspection of former datasets.We
      employ a Visual Analytics framework for generating and visualizing data
      provenance from data quality to facilitate data analysis and help users
      to understand the impact of updates on the data. Data quality metrics are
      utilized to quantify the development of data quality over time for open
      data projects. We combine quality metrics, data provenance, and data
      transformation information in an interactive exploration environment to
      expedite assessment and selection of appropriate open datasets.
    </div>

    <div class="title">
      Case Studies of Shareable Personal Map Visualization
    </div>

    <div class="links">
      [<a href="pdf/013-015.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="013-015.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Puripant Ruchikachorn
    </div>

    <div class="button">
      <button onclick="myFunction(4)">+/-</button>
    </div>

    <div id='4' class="abstract">
      This paper presents two examples of personal data visualizations to be
      shared among peers. The visualized and shared data were travel
      destinations in Thailand and daily commutes in Bangkok, Thailand. The
      former gathered much attention with almost a million visitors within the
      first week after launch or approximately 2% of internet users in
      Thailand. Despite minimal data collection, large data samples of the
      first case study enable various analyses. The easy-to-use interfaces and
      simple visualizations can be a model of the genre of personal
      visualization whose main task is to share.
    </div>

    <div class="title">
      An Eye-Tracking Study on Sparklines within Textual Context
    </div>

    <div class="links">
      [<a href="pdf/017-019.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="017-019.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Puripant Ruchikachorn and Pimmanee Rattanawicha
    </div>

    <div class="button">
      <button onclick="myFunction(5)">+/-</button>
    </div>

    <div id='5' class="abstract">
      Sparklines are placed in documents but their usability is rarely
      evaluated in their immediate context of paragraphs of text. We conducted
      an eye-tracking study to measure readability and understandability of
      four different conditions: two different sparkline chart types (bar and
      line charts) and two text languages (native and non-native languages). We
      found out that most participants out of 296 in total were not distracted
      by sparklines. Only 3.19% of the average reading time was spent looking
      at sparklines. There was no correlation between dwell time and data
      understanding, measured in a post-experiment quiz. The chart types did
      not have a significant effect on sparkline attention. However, compared
      with native textual context, sparklines in non-native text were more
      noticeable. The results of this study can be useful for future sparkline
      usage consideration.
    </div>

    <div class="title">
      Network Analysis for Financial Fraud Detection
    </div>

    <div class="links">
      [<a href="pdf/021-023.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="021-023.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Roger A. Leite, Theresia Gschwandtner, Silvia Miksch, Erich Gstrein, and
      Johannes Kuntner
    </div>

    <div class="button">
      <button onclick="myFunction(6)">+/-</button>
    </div>

    <div id='6' class="abstract">
      Security and quality are main concerns for private and public financial
      institutions. Data mining techniques based on the profiles of customers
      of a financial institution are commonly used to avoid fraud and financial
      damage. However, these approaches often are limited to the analysis of
      individual customers which hinders the detection of fraudulent networks.
      We propose a Visual Analytics approach for supporting and fine-tuning
      customers' network analysis, thus, reducing false-negative alarms of
      frauds.
    </div>

    <div class="title">
      Validation of Quantitative Measures for Edge Bundling by Comparing with
      Human Feeling
    </div>

    <div class="links">
      [<a href="pdf/025-027.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="025-027.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Ryosuke Saga
    </div>

    <div class="button">
      <button onclick="myFunction(7)">+/-</button>
    </div>

    <div id='7' class="abstract">
      This paper describes an analysis of the relationship between human
      cognition and quantitative measures for a visualization method called
      edge bundling.Aesthetic rules-based measures, namely, mean edge length
      difference (MELD), normalized MELD (NMELD), mean occupation area, and
      edge density distribution (EDD), for evaluating and quantifying the
      result of edge bundling are proposed. However, comparing these measures
      with human cognition has not been analyzed. Therefore, a questionnaire
      survey with approximately 40 respondents was conducted to clarify the
      relationship between human cognition and these evaluation measures.
      Results showed that NMELD, MELD, and EDD demonstrate robust and
      significant correlations with human cognition.
    </div>

    <div class="title">
      ViMEC: Interactive Application for Micro-Cluster Visualizations
    </div>

    <div class="links">
      [<a href="pdf/029-031.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="029-031.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Florian Schmidt and Yannick Ehrenfeld
    </div>

    <div class="button">
      <button onclick="myFunction(8)">+/-</button>
    </div>

    <div id='8' class="abstract">
      Digitalization increases the opportunity to collect vast amounts of data
      in a large scale manner. In order to handle the information overload,
      data mining techniques like online clustering are performed. A lot of
      online clusterers are based on the concept of micro-clusters in order to
      represent the given data stream. Based on its definition, micro-clusters
      can be represented as an n-sphere. Online clustering algorithms like
      BIRCH or DenStream use different strategies for maintaining the
      micro-clusters in evolving time series, but using the same underlying key
      concept storing a summarized version of the data stream in their models.
      We propose ViMEC, an application for multidimensional micro-cluster
      visualization, giving the user the opportunity to gain understanding of
      the internal behaviour of the clustering model. For a given time frame,
      ViMEC gives the user three different types of visualizations presenting
      different levels of details: Overview, Pair-view and Detail-view. These
      views combine not only a summary and detail representations for the
      different dimensions, but also aim to show different relations between
      dimensions. Preliminary results show, that large data sets with up to
      20,000 data points can be visualized within less than 20 seconds.
    </div>

    <div class="title">
      Exploring Uncertainty in Image Segmentation Ensembles
    </div>

    <div class="links">
      [<a href="pdf/033-035.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="033-035.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Bernhard Fr&ouml;hler, Torsten M&ouml;ller, Johannes Weissenb&ouml;ck,
      Hans-Christian Hege, Johann Kastner, and Christoph Heinzl
    </div>

    <div class="button">
      <button onclick="myFunction(9)">+/-</button>
    </div>

    <div id='9' class="abstract">
      Finding the most accurate image segmentation involves analyzing results
      from different algorithms or parameterizations. In this work, we identify
      different types of uncertainty in this analysis that are represented by
      the results of probabilistic algorithms, by the local variability in the
      segmentation, and by the variability across the segmentation ensemble. We
      propose visualization techniques for the analysis of such types of
      uncertainties in segmentation ensembles. For a global analysis we provide
      overview visualizations in the image domain as well as in the label
      space. Our probability probing and scatter plot based techniques
      facilitate a local analysis. We evaluate our techniques using a case
      study on industrial computed tomography data.
    </div>

    <div class="title">
      Supporting Visual Parameter Analysis of Time Series Segmentation with
      Correlation Calculations
    </div>

    <div class="links">
      [<a href="pdf/037-039.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="037-039.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Christian Eichner, Heidrun Schumann, and Christian Tominski
    </div>

    <div class="button">
      <button onclick="myFunction(10)">+/-</button>
    </div>

    <div id='10' class="abstract">
      Parameter analysis can be used to find out how individual parameters
      influence the output of an algorithm. We aim to support the visual
      parameter analysis of algorithms for the segmentation of time series. To
      this end, we automatically search for correlations between parameters and
      the segmentation outputs. Correlations are not only determined globally,
      but also locally within parameter subspaces. Calculated correlations are
      used to visually emphasize parameter and value ranges with high influence
      on the segmentation. By interactive exploration, the analyst can study
      the multidimensional parameter space in depth.
    </div>

    <div class="title">
      The Impact of Visualizing Uncertainty on Train Trip Selection
    </div>

    <div class="links">
      [<a href="pdf/041-043.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="041-043.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Marcel Wunderlich, Kathrin Ballweg, and Tatiana von Landesberger
    </div>

    <div class="button">
      <button onclick="myFunction(11)">+/-</button>
    </div>

    <div id='11' class="abstract">
      Train trip planning means deciding on one of several travel connections.
      Possible train delays lead to uncertainties in the schedule connections
      and may influence the planning decisions. Although several designs for
      the visualization of the available train trips exist, it is still unclear
      how these designs and the visualization of delay uncertainty influence
      the decision making. We let 86 people decide ten times on different train
      trips using one of four designs, (not) visualizing delay uncertainty and
      with(out) temporal constraints. The results show that planning decisions
      depend on whether the design is visual or textual and on the availability
      of trip uncertainty visualization. In case of a temporal constraint,
      non-critical train connections are preferred.
    </div>

    <div class="title">
      Categorizing Uncertainties in the Process of Segmenting and Labeling Time
      Series Data
    </div>

    <div class="links">
      [<a href="pdf/045-047.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="045-047.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Markus B&ouml;gl, Christian Bors, Theresia Gschwandtner, and Silvia
      Miksch
    </div>

    <div class="button">
      <button onclick="myFunction(12)">+/-</button>
    </div>

    <div id='12' class="abstract">
      The segmenting and labeling of multivariate time series data is applied
      in different domains, e.g. activity recognition or sensor states. This
      involves several steps of (pre-) processing, segmenting, and labeling of
      time intervals, and visually exploring the results as well as iteratively
      refining the parameters for all the processing steps. Within these
      processes different uncertainties are involved and relevant. In this
      poster we identify and categorize important uncertainties in this problem
      domain. We discuss challenges for visually communicating these
      uncertainties throughout the segmenting and labeling process.
    </div>

    <div class="title">
      Visual Analysis of Sentiment and Stance in Social Media Texts
    </div>

    <div class="links">
      [<a href="pdf/049-051.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="049-051.pdf.html">meta data
      <img src="../images/metadata.gif" border="0" height="16" width="16">]
      [multimedia</a> <img src="../images/movie.gif" border="0" height="16"
      width="16">]
    </div>

    <div class="author">
      Kostiantyn Kucher, Carita Paradis, and Andreas Kerren
    </div>

    <div class="button">
      <button onclick="myFunction(13)">+/-</button>
    </div>

    <div id='13' class="abstract">
      Despite the growing interest for visualization of sentiments and emotions
      in textual data, the task of detecting and visualizing various stances is
      not addressed well by the existing approaches. The challenges associated
      with this task include development of the underlying computational
      methods and visualization of the corresponding multi-label stance
      classification results. In this poster abstract, we describe the ongoing
      work on a visual analytics platform, called StanceVis Prime, which is
      designed for analysis of sentiment and stance in temporal text data from
      various social media data sources. Our approach consumes documents from
      several text stream sources, applies sentiment and stance classification,
      and provides end users with both an overview of the resulting data series
      and a detailed view for close reading and examination of the classifiers'
      output. The intended use case scenarios for StanceVis Prime include
      social media monitoring and research in sociolinguistics.
    </div>

    <div class="title">
      Towards Natural Language Empowered Interactive Data Analysis
    </div>

    <div class="links">
      [<a href="pdf/053-055.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="053-055.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Cagatay Turkay and Rafael Henkin
    </div>

    <div class="button">
      <button onclick="myFunction(14)">+/-</button>
    </div>

    <div id='14' class="abstract">
      The recent advances in natural language based interaction methodologies
      offer promising avenues to enhance the interactive processes within the
      human-machine dialogue of visual analytics. We envisage Multimodal Data
      Analytics as a novel approach for conducting data analysis that builds on
      the strengths of visual analytics and natural language as an expressive
      interaction channel. We investigate the potential enhancements from such
      a multimodal approach and discusses the preliminary outline for a
      structured methodology to study the role of natural language in data
      analytics. Our approach builds on a simple model of human machine
      dialogue for interactive data analysis which we then propose to
      instantiate as visual analytics workflows - representations to study and
      operationalise interactive data analysis routines empowered by natural
      language interaction.
    </div>

    <div class="title">
      A Visual Comparison of Hand-Drawn and Machine-Generated Human Metabolic
      Pathways
    </div>

    <div class="links">
      [<a href="pdf/057-059.pdf">full paper</a> <img src="../images/pdf.gif"
      border="0" height="16" width="16">] [<a href="057-059.pdf.html">meta
      data</a> <img src="../images/metadata.gif" border="0" height="16" width=
      "16">]
    </div>

    <div class="author">
      Hsiang-Yun Wu, Martin N&ouml;llenburg, and Ivan Viola
    </div>

    <div class="button">
      <button onclick="myFunction(15)">+/-</button>
    </div>

    <div id='15' class="abstract">
      This poster abstract presents a visual comparison between three
      hand-drawn and one machine-generated human metabolic pathway diagrams.
      The human metabolic pathways, which describe significant biochemical
      reactions in the human body, have been increasingly investigated due to
      the development of analysis processes and are compiled into pathway
      diagrams to provide an overview of reaction in human body. This complex
      network includes about 5;000 metabolites and 7;500 reactions, which are
      hierarchically nested and difficult to visualize. We collect and analyze
      well-known human metabolic pathway diagrams, and summarize the design
      choices of these diagrams, respectively. Together with a
      machine-generated diagram, we can understand the visual complexity of
      three hand-drawn and one machine-generated diagrams.
    </div>
  </div>
</body>
</html>
